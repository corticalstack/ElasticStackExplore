# Purpose: Ingest news in csv format from Kaggle dataset and load into ES 
# Author: Jon-Paul Boyd

# Ingest wildcard articles csv files
input {
  file {
			path => "c:/logstash-6.5.4/in/csv/articles*.csv"
			mode => "read"
			start_position => "beginning"
			file_chunk_size => 165000
		}
}

filter {
  csv {
    separator => ","
    columns => ["counter", "id", "title", "publication", "author", "date", "year", "month", "url", "content"]
  }
  
# Remove header line  
  if [id] == "id" {
	drop {}
	}	

# Persist kaggle record id in metadata for use in output in document_id (as no need for id in field)	
# Persist dataset source, and cleanse title by removing publication as persisted in own field
  mutate {
	add_field => { "[@metadata][id]" => "%{id}" }
	add_field => { "dataset" => "kaggle" }
	gsub => ["title", "\ -.*", ""]   
	gsub => ["year", "\..*", ""]   
	gsub => ["month", "\..*", ""]   
	gsub => ["title", "\ -.*", ""]   
	}
	
# Add yearmonth, then map source to target fields	
  mutate {
	add_field => { "yearmonth" => "%{year}%{month}" }
	rename => { "date" => "date_publication" }
	rename => { "content" => "body" }
	}

  mutate {
	remove_field => ["counter", "id", "month", "@timestamp", "message", "path", "url", "host"]
  }
}

output {
  elasticsearch {
    hosts => "https://ab93385654d74a0da876074a41d0c243.eu-central-1.aws.cloud.es.io:9243/"
    user => "*****"
    password => "*****"
    index => "news_%{[year]}"
	document_id => "kaggle_%{[@metadata][id]}"
	}
	stdout { codec => rubydebug }
} 